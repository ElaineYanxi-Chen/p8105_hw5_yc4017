Homework 5
================
Elaine Yanxi Chen
2022-11-16

## Packages and settings

First we load the packages necessary to knit this document.

``` r
library(tidyverse)
library(p8105.datasets)
library(viridis)

set.seed(1)

knitr::opts_chunk$set(
    echo = TRUE,
    warning = FALSE,
    fig.width = 8, 
  fig.height = 6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

## Problem 1

The study data for each participant is included in a separate file which
include the subject ID and arm. We want to create a tidy dataframe with
data from all participants, including the subject ID, arm, and
observations over time.

First, we want a dataframe with all file names. After, we map the
`read_csv` function over each path name, which creates a list column.
Lastly, we use `unnest` to expand the list column.

``` r
full_df = 
  tibble(
    files = list.files("data/data/"),
    path = str_c("data/data/", files)
  ) %>% 
  mutate(data = map(path, read_csv)) %>% 
  unnest()
```

Now we need to tidy the result. And file names to include control arm
and subject ID. Tidy weekly observations and perform other tidying as
needed.

``` r
tidy_df = 
  full_df %>% 
  mutate(
    files = str_replace(files, ".csv", ""),
    group = str_sub(files, 1, 3)
  ) %>% 
  pivot_longer(
    week_1:week_8,
    names_to = "week",
    values_to = "outcome",
    names_prefix = "week_"
  ) %>% 
  select(group, subj = files, week, outcome)
```

Lastly, we will make a spaghetti plot for observations on each subject
over time.

``` r
tidy_df %>% 
  ggplot(aes(x = week, y = outcome, group = subj, colour = group)) +
  geom_point() +
  geom_path() +
  facet_grid(~group)
```

<img src="p8105_hw5_yc4017_files/figure-gfm/unnamed-chunk-3-1.png" width="90%" />

Comparing the two groups, we see that the average outcome is much lower
in the control group than the experimental group. In addition, while
outcome for the control group remained fairly constant, the outcome for
the experimental group increased steadily from week 1 to week 8.

## Problem 2: Homicide rates

This dataset contains information on more than 52,000 criminal homicides
over the past decade in 50 of the largest American cities. The
information include the location and arrest information on each killing,
along with basic demogrpahic information about each victim. We first
download the data from the github repo and import it.

``` r
homicide_raw = read_csv(file = "data/homicide-data.csv")
```

    ## Rows: 52179 Columns: 12
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## chr (9): uid, victim_last, victim_first, victim_race, victim_age, victim_sex...
    ## dbl (3): reported_date, lat, lon
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

The raw data contains 52179 rows and 12 columns. It has variables on the
case ID, date, location including city, state, latitude, and longtitude,
victim information including race, age, sex, and disposition.

Next, we want to create a `city_state` variable to summarize within
cities the total number of homicides and the number of unsolved
homicides.

``` r
homicide_raw = homicide_raw %>% 
  janitor::clean_names() %>% 
  mutate(
    city_state = str_c(city, state, sep = ","),
    status = 
      case_when(
      disposition == "Closed without arrest" ~ "unsolved",
      disposition == "Open/No arrest" ~ "unsolved",
      disposition == "Closed by arrest" ~ "solved"
    )) 

homicide_city = homicide_raw %>% 
  group_by(city_state) %>% 
  summarize(
    total = n(),
    unsolved = sum(status == "unsolved")
  )
```

Now we want to use the `prop.test` function to estimate the proportion
of homicides unsolved in Baltimore, MD. We will also pull the estimate
and the 95% confidence interval of the proportion of unresolved
homicides in Baltimore, MD, from the proportion test results.

``` r
prop_balt = prop.test(
  x = homicide_city %>% filter(city_state == "Baltimore,MD") %>% pull(unsolved),
  n = homicide_city %>% filter(city_state == "Baltimore,MD") %>% pull(total), 
  alternative = c("two.sided"),
  conf.level = 0.95,
  correct = TRUE) 

prop_balt %>% 
  saveRDS(., file = "data/prop_test_balt.rds")

prop_balt %>% broom::tidy() %>% select(c(estimate, conf.low, conf.high))
```

    ## # A tibble: 1 × 3
    ##   estimate conf.low conf.high
    ##      <dbl>    <dbl>     <dbl>
    ## 1    0.646    0.628     0.663

Now we need to run the same test for each of the cities in the dataset
and extract the proportion of unsolved homicides and the confidence
interval for each. To do so, we could `map` the `prop.test` function
over the unsolved and total number of homicides of each city. After, we
pipe the result into a clean dataset by using the `broom::tidy()`
function. We then `unnest` the list column from the tidied dataset and
select only the esetimate and confidence intervals that we are
interested in for each `city_state`.

``` r
homicide_prop = 
  homicide_city %>% 
  mutate(
    proportions = map2(.x = unsolved, .y = total, ~prop.test(x = .x, n = .y)),
    tidied = map(.x = proportions, ~broom::tidy(.x))
  ) %>% 
  unnest(tidied) %>% 
  select(city_state, estimate, conf.low, conf.high)
```

Now we need to create a plot that shows the estimates and CIs for each
city. We can use `geom_errorbar` to add error bars based on the upper
and lower limits. Lastly, we organize the cities according to the
porpotion of unsolved homicides.

``` r
homicide_plot = homicide_prop %>% 
  ggplot(aes(x = fct_reorder(city_state, estimate), y = estimate)) + 
  geom_point() +  
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high)) +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))

homicide_plot
```

<img src="p8105_hw5_yc4017_files/figure-gfm/unnamed-chunk-8-1.png" width="90%" />

## Problem 3: Simulation

We will run a simulation to explore power in one-sample t-test.

First set design elements with n = 30 and sigma = 5.

Then set mu = 0 and generate 5000 datasets from the normal distribution
model.

``` r
sim_data = function(n = 30, mu = 0, sigma = 5){
  
    x = rnorm(n, mean = mu, sd = sigma)
  
  t_test = t.test(x, conf.int = 0.95) %>% broom::tidy() %>% select(estimate, p.value)
  
  t_test
  
}

output = vector("list", 5000)

for (i in 1:5000) {
  
 output[[i]] = sim_data()
    
}

output %>% bind_rows()
```

    ## # A tibble: 5,000 × 2
    ##    estimate p.value
    ##       <dbl>   <dbl>
    ##  1    0.412  0.629 
    ##  2    0.664  0.368 
    ##  3    0.551  0.534 
    ##  4    0.567  0.487 
    ##  5   -1.65   0.0599
    ##  6    1.19   0.229 
    ##  7    0.334  0.738 
    ##  8   -1.19   0.209 
    ##  9    0.122  0.887 
    ## 10    0.684  0.472 
    ## # … with 4,990 more rows

Now wee want to repeat the above for mu = {1, 2, 3, 4, 5, 6}

``` r
sim_mu = function(set){
  output = vector("list", 5000)
  for (i in 1:5000) {
    
     output[[i]] = sim_data(mu = set)
     
  }
  
power = 
    output %>% 
    bind_rows() 
  
  power
  
}

powertest =
  tibble(
    sample_mu = c(0, 1, 2, 3, 4, 5, 6),
    reject = map(sample_mu, sim_mu)
  ) %>%  
  unnest(reject) %>% 
  mutate(status = ifelse(p.value < 0.05, TRUE, FALSE))
```

Next we want to make a plot showing the proportion of times the null was
rejected, the power of the test, on the y-axis and the true value of mu
on the x axis.

``` r
power_plot = powertest %>% 
  group_by(sample_mu) %>% 
  summarize(power = sum(status)/5000) %>% 
  ggplot(aes(x = sample_mu, y = power)) +
  geom_point(aes(colour = sample_mu), size = 2, alpha = 0.5) +
  geom_line(alpha = 0.3) +
  labs(title = "The Power of one-sample t-test vs. true value of mu",
       x = "True value of mu", 
       y = "Power of the one-sample t-test")
power_plot
```

<img src="p8105_hw5_yc4017_files/figure-gfm/unnamed-chunk-11-1.png" width="90%" />

Based on the plot we can see that the power increases as the effect size
increases.

Now we want to make a plot showing the average estimate of mu on the y
axis and the true value of mu on the x axis.

We want another plot showing the average estimate of mu only in samples
for which null was rejected on the y axis.

``` r
all_estimates = 
  powertest %>%
  group_by(sample_mu) %>% 
  summarize(all_samp_mean = mean(estimate))

reject_estimates =
  powertest %>%
  group_by(sample_mu) %>% 
  filter(status == TRUE) %>%
  summarize(reject_samp_mean = mean(estimate))

combined_df = 
  full_join(all_estimates, reject_estimates, by = "sample_mu") %>%
  pivot_longer(
    all_samp_mean:reject_samp_mean, 
    names_to = "samples",
    values_to = "avg_estimates"
  )

all_plot = 
  all_estimates %>%
  ggplot(aes(x = sample_mu, y = all_samp_mean)) +
  geom_point(alpha = 0.5) +
  geom_line() +
  labs(title = "All Sample",
       x = "True value of μ",
       y = "The average estimate of μ^")
all_plot
```

<img src="p8105_hw5_yc4017_files/figure-gfm/unnamed-chunk-12-1.png" width="90%" />

``` r
reject_plot = 
  reject_estimates %>%
  ggplot(aes(x = sample_mu, y = reject_samp_mean)) +
  geom_point(alpha = 0.5) +
  geom_line() +
  labs(title = "Rejected-Null Sample",
       x = "True value of μ",
       y = "The average estimate of μ^")
 reject_plot
```

<img src="p8105_hw5_yc4017_files/figure-gfm/unnamed-chunk-12-2.png" width="90%" />

``` r
combined_plot = 
  combined_df %>%
  ggplot(aes(x = sample_mu, y = avg_estimates, group = samples)) +
  geom_point(aes(colour = samples), alpha = 0.5) +
  geom_line(aes(colour = samples)) +
  labs(title = "True Value of μ vs.Average Estimate of μ^ in All Sample and Rejected-Null Sample",
         x = "True population mean (μ)",
         y = "The average estimate of μ^")
combined_plot
```

<img src="p8105_hw5_yc4017_files/figure-gfm/unnamed-chunk-12-3.png" width="90%" />

-   From the plot above, we can observe that the sample average of μ̂
    across tests for which the null is rejected is approximately equal
    to the true value of μ, when μ lies between 0 and 4.
-   It is evident that μ̂ \>μ in these cases.
-   When μ =\> 4, then we see that μ̂ approximates μ.
-   This is because as effect size increases, the power also increases.
