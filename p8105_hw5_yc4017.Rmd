---
title: "Homework 5"
author: "Elaine Yanxi Chen"
date: "`r Sys.Date()`"
output: github_document
---

## Packages and settings

First we load the packages necessary to knit this document.

```{r packages and settings, message = FALSE}
library(tidyverse)
library(p8105.datasets)
library(viridis)

set.seed(1)

knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
	fig.width = 8, 
  fig.height = 6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```


## Problem 1

The study data for each participant is included in a separate file which include the subject ID and arm. We want to create a tidy dataframe with data from all participants, including the subject ID, arm, and observations over time.

First, we want a dataframe with all file names. After, we map the `read_csv` function over each path name, which creates a list column. Lastly, we use `unnest` to expand the list column.

```{r, message = FALSE}
full_df = 
  tibble(
    files = list.files("data/data/"),
    path = str_c("data/data/", files)
  ) %>% 
  mutate(data = map(path, read_csv)) %>% 
  unnest()
```

Now we need to tidy the result. And file names to include control arm and subject ID. Tidy weekly observations and perform other tidying as needed.

```{r}
tidy_df = 
  full_df %>% 
  mutate(
    files = str_replace(files, ".csv", ""),
    group = str_sub(files, 1, 3)
  ) %>% 
  pivot_longer(
    week_1:week_8,
    names_to = "week",
    values_to = "outcome",
    names_prefix = "week_"
  ) %>% 
  select(group, subj = files, week, outcome)
```


Lastly, we will make a spaghetti plot for observations on each subject over time. 

```{r}
tidy_df %>% 
  ggplot(aes(x = week, y = outcome, group = subj, colour = group)) +
  geom_point() +
  geom_path() +
  facet_grid(~group)
```


Comparing the two groups, we see that the average outcome is much lower in the control group than the experimental group. In addition, while outcome for the control group remained fairly constant, the outcome for the experimental group increased steadily from week 1 to week 8.



## Problem 2: Homicide rates


This dataset contains information on more than 52,000 criminal homicides over the past decade in 50 of the largest American cities. The information include the location and arrest information on each killing, along with basic demogrpahic information about each victim. We first download the data from the github repo and import it.

```{r}
homicide_raw = read_csv(file = "data/homicide-data.csv")
```


The raw data contains `r nrow(homicide_raw)` rows and `r ncol(homicide_raw)` columns. It has variables on the case ID, date, location including city, state, latitude, and longtitude, victim information including race, age, sex, and disposition. 

Next, we want to create a `city_state` variable to summarize within cities the total number of homicides and the number of unsolved homicides. 


```{r}
homicide_raw = homicide_raw %>% 
  janitor::clean_names() %>% 
  mutate(
    city_state = str_c(city, state, sep = ","),
    status = 
      case_when(
      disposition == "Closed without arrest" ~ "unsolved",
      disposition == "Open/No arrest" ~ "unsolved",
      disposition == "Closed by arrest" ~ "solved"
    )) 

homicide_city = homicide_raw %>% 
  group_by(city_state) %>% 
  summarize(
    total = n(),
    unsolved = sum(status == "unsolved")
  )
```


Now we want to use the `prop.test` function to estimate the proportion of homicides unsolved in Baltimore, MD.

Need to pull the estimated proportion and confidence intervals from the resulting tidy dataframe.

```{r}
prop.test(
  x = homicide_city %>% filter(city_state == "Baltimore,MD") %>% pull(unsolved),
  n = homicide_city %>% filter(city_state == "Baltimore,MD") %>% pull(total)) %>% 
  saveRDS(., file = "data/prop_test_balt.rds")

readRDS(file = "data/prop_test_balt.rds") %>% broom::tidy() %>% pull(estimate)
```


Now we need to run the same test for each of the cities in the dataset and extract the porportion of unsolved homicides and the confidence interval for each.

```{r}
homicide_prop = 
  homicide_city %>% 
  mutate(
    proportions = map2(.x = unsolved, .y = total, ~prop.test(x = .x, n = .y)),
    tidied = map(.x = proportions, ~broom::tidy(.x))
  ) %>% 
  select(-proportions) %>% 
  unnest(tidied) %>% 
  select(city_state, estimate, conf.low,conf.high)
```


Now we need to create a plot that shows the estimates and CIs for each city. We can use `geom_errorbar` to add error bars based on the upper and lower limits. Lastly, we organize the cities according to the porpotion of unsolved homicides. 

```{r}
homicide_plot = homicide_prop %>% 
  ggplot(aes(x = fct_reorder(city_state, estimate), y = estimate)) + 
  geom_point() +  
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high)) +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))

homicide_plot
```


## Problem 3: Simulation

We will run a simulation to explore power in one-sample t-test.

First set design elements with n = 30 and sigma = 5.

Then set mu = 0 and generate 5000 datasets from the normal distribution model. 

```{r}
sim_data = function(n = 30, mu = 0, sigma = 5){
  
    x = rnorm(n, mean = mu, sd = sigma)
  
  t_test = t.test(x, conf.int = 0.95) %>% broom::tidy() %>% select(estimate, p.value)
  
  t_test
  
}

output = vector("list", 5000)

for (i in 1:5000) {
  
 output[[i]] = sim_data()
    
}

output %>% bind_rows()
```


Now wee want to repeat the above for mu = {1, 2, 3, 4, 5, 6}

```{r}
sim_mu = function(set){
  output = vector("list", 5000)
  for (i in 1:5000) {
    
     output[[i]] = sim_data(mu = set)
     
  }
  
power = 
    output %>% 
    bind_rows() 
  
  power
  
}

powertest =
  tibble(
    sample_mu = c(0, 1, 2, 3, 4, 5, 6),
    reject = map(sample_mu, sim_mu)
  ) %>%  
  unnest(reject) %>% 
  mutate(status = ifelse(p.value < 0.05, TRUE, FALSE))
```


Next we want to make a plot showing the proportion of times the null was rejected, the power of the test, on the y-axis and the true value of mu on the x axis.


```{r}
power_plot = powertest %>% 
  group_by(sample_mu) %>% 
  summarize(power = sum(status)/5000) %>% 
  ggplot(aes(x = sample_mu, y = power)) +
  geom_point(aes(colour = sample_mu), size = 2, alpha = 0.5) +
  geom_line(alpha = 0.3) +
  labs(title = "The Power of one-sample t-test vs. true value of mu",
       x = "True value of mu", 
       y = "Power of the one-sample t-test")
power_plot
```

Based on the plot we can see that the power increases as the effect size increases.


Now we want to make a plot showing the average estimate of mu on the y axis and the true value of mu on the x axis.

We want another plot showing the average estimate of mu only in samples for which null was rejected on the y axis.


```{r}
all_estimates = 
  powertest %>%
  group_by(sample_mu) %>% 
  summarize(all_samp_mean = mean(estimate))

reject_estimates =
  powertest %>%
  group_by(sample_mu) %>% 
  filter(status == TRUE) %>%
  summarize(reject_samp_mean = mean(estimate))

combined_df = 
  full_join(all_estimates, reject_estimates, by = "sample_mu") %>%
  pivot_longer(
    all_samp_mean:reject_samp_mean, 
    names_to = "samples",
    values_to = "avg_estimates"
  )

all_plot = 
  all_estimates %>%
  ggplot(aes(x = sample_mu, y = all_samp_mean)) +
  geom_point(alpha = 0.5) +
  geom_line() +
  labs(title = "All Sample",
       x = "True value of μ",
       y = "The average estimate of μ^")
all_plot

reject_plot = 
  reject_estimates %>%
  ggplot(aes(x = sample_mu, y = reject_samp_mean)) +
  geom_point(alpha = 0.5) +
  geom_line() +
  labs(title = "Rejected-Null Sample",
       x = "True value of μ",
       y = "The average estimate of μ^")
 reject_plot
 
combined_plot = 
  combined_df %>%
  ggplot(aes(x = sample_mu, y = avg_estimates, group = samples)) +
  geom_point(aes(colour = samples), alpha = 0.5) +
  geom_line(aes(colour = samples)) +
  labs(title = "True Value of μ vs.Average Estimate of μ^ in All Sample and Rejected-Null Sample",
         x = "True population mean (μ)",
         y = "The average estimate of μ^")
combined_plot
```


- From the plot above, we can observe that the sample average of μ̂ across tests for which the null is rejected is approximately equal to the true value of μ, when μ lies between 0 and 4. 
- It is evident that  μ̂ >μ in these cases. 
- When μ => 4, then we see that μ̂ approximates μ. 
- This is because as effect size increases, the power also increases. 